{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouvelle section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des bibliothèques, définition des fonctions appelées dans notre code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MM41KgXR78Le"
   },
   "outputs": [],
   "source": [
    "# import des bibliothèques nécessaires\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Masking, Dropout,TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TjDuEdS8PWSr"
   },
   "outputs": [],
   "source": [
    "# fonction pour lire le fichier csv d'entrainement et le mettre dans un dataframe\n",
    "def read_file(name):\n",
    "    with open(name, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        data_brut = []\n",
    "        for line in lines:\n",
    "            # on verifie que la ligne commence bien par http\n",
    "            if line.startswith('http'):\n",
    "                values = line.strip().split(\",\")\n",
    "                data_brut.append(values)\n",
    "            else : continue\n",
    "    df = pd.DataFrame(data_brut)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-J6OvwjaPbI6"
   },
   "outputs": [],
   "source": [
    "# fonction qui remplace toute les valeurs None d'un dataframe par la chaine de caractere 'vide'\n",
    "def val_nulle(df):\n",
    "    for i in range(len(df.columns)):\n",
    "        df.iloc[:, i] = df.iloc[:, i].fillna('vide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pZfzivodqejH"
   },
   "outputs": [],
   "source": [
    "# fonction qui enleve les valeur qui commencent par un 't' car elles ne sont pas exploitées\n",
    "def sans_t(df,lettre):\n",
    "    # stocakge des valeurs commencent par un 't' dans une variable\n",
    "    mots_avec_t = df.stack().loc[lambda x: x.apply(lambda y: y.startswith(lettre))].unique()\n",
    "\n",
    "    # parcours des valeurs du dataframe et remplacement par None pour chaque valeur étant dans la variable 'mots_avec_t'\n",
    "    df = df.stack().apply(lambda x: None if x in mots_avec_t else x).unstack()\n",
    "    \n",
    "    # creation d'un dataframe de la meme taille que celui prit en parametre, rempli de valeurs NaN\n",
    "    nan_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    nan_df[:] = np.nan\n",
    "\n",
    "    # parcourt du dataframe en parametre\n",
    "    for index, row in df.iterrows():\n",
    "        # pour chaque ligne, on definit une liste qui garde les valeurs non nulle et une qui prend les valeurs nulles\n",
    "        new_row = []  # liste valeur non nulle\n",
    "        none_values = []  # liste valeur None\n",
    "        # parcourt des valeurs de la ligne\n",
    "        for value in row:\n",
    "            # la valeur est mise dans une liste en fonction de sa valeur\n",
    "            if pd.notna(value):\n",
    "                new_row.append(value)\n",
    "            else:\n",
    "                none_values.append(np.nan) \n",
    "        # ajouter des valeurs None à la fin de la ligne pour conserver les memes dimensions\n",
    "        new_row.extend(none_values)\n",
    "        # remplir les valeurs restantes avec NaN si besoin\n",
    "        new_row.extend([np.nan] * (len(df.columns) - len(new_row)))\n",
    "        # mettre la nouvelle ligne dans le nouveau dataframe\n",
    "        nan_df.loc[index] = new_row\n",
    "\n",
    "    return nan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Szyd_GcuG5XQ"
   },
   "outputs": [],
   "source": [
    "# fonction pour redimensionner le dataframe, en enlevant les dernieres colonnes qui ne contiennent que des valeurs nulles\n",
    "def reduction(df):\n",
    "    df_avant_encodage = pd.DataFrame(df.iloc[:,:(df.shape[1])-df.isna().sum(axis=1).min()])\n",
    "    return df_avant_encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nYaXfq99H9gu"
   },
   "outputs": [],
   "source": [
    "# création du dictionnaire\n",
    "def creation_dico(df):\n",
    "    # création du'une liste et d'un compteur\n",
    "    valeurs_uniques = {}\n",
    "    compteur = 0\n",
    "\n",
    "    # parcourt de chaque valeur du dataframe\n",
    "    for col in df.columns:\n",
    "        for valeur in df[col]:\n",
    "            # ajouter la valeur dans le dictionnaire si elle n'y est pas deja\n",
    "            if valeur not in valeurs_uniques:\n",
    "                valeurs_uniques[valeur] = compteur\n",
    "                compteur += 1\n",
    "\n",
    "    return valeurs_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Z64-5rvzIuXX"
   },
   "outputs": [],
   "source": [
    "# encodage avec le dataframe et le dictionnaire\n",
    "def encodage(df,dictionnaire):\n",
    "    # creation d'une copie du dataframe\n",
    "    df_encoded = df.copy()\n",
    "    # encodage de la copie avec le dictionnaire\n",
    "    for col in df_encoded.columns:\n",
    "        df_encoded[col] = df_encoded[col].replace(dictionnaire)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RXlURlQULwLG"
   },
   "outputs": [],
   "source": [
    "# fonction pour reencoder certaines colonnes defectueuses\n",
    "def correction(df_avant,df_apres,dictionnaire):\n",
    "    # recupere le type de chaque colonne\n",
    "    types_de_donnees = df_avant.dtypes\n",
    "\n",
    "    # separation entre les colonnes de int et de float\n",
    "    colonnes_int = types_de_donnees[types_de_donnees == 'int64'].index\n",
    "    colonnes_float = types_de_donnees[types_de_donnees == 'float64'].index\n",
    "\n",
    "    # reencodage des colonnes float\n",
    "    for colonne in colonnes_float:\n",
    "        df_apres[colonne] = df_avant[colonne].map(dictionnaire)\n",
    "    return df_apres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LbgBBFtTNMG7"
   },
   "outputs": [],
   "source": [
    "# fonction SMOTE, pour ajouter des enchatillons aux classes sous-representées\n",
    "def smote(df_y,df_x):\n",
    "    # mise du dataframe target dans une liste\n",
    "    Y_list = df_y.tolist()\n",
    "\n",
    "    # comptage du nombre d'echantillons par classe\n",
    "    class_count = Counter(Y_list)\n",
    "\n",
    "    # oversampling pour atteindre 50 echantillons par classe\n",
    "    sampling_strategy = {label: 50 for label, count in class_count.items() if count < 50}\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, k_neighbors=3)\n",
    "    X_smote, Y_smote = smote.fit_resample(df_x, df_y)\n",
    "\n",
    "    # reunion des dataframes target et feature\n",
    "    XY_smote = pd.concat([Y_smote,X_smote], axis = 1)\n",
    "    return XY_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jbWLFrMhOBj4"
   },
   "outputs": [],
   "source": [
    "# undersampling des classes sur-representées\n",
    "def under(df):\n",
    "    # compte du nombre d'echantillons par classe\n",
    "    class_list = df[0].value_counts()\n",
    "\n",
    "    # reduction du nombre d'echantillons a 50\n",
    "    df = pd.concat([df[df[0] == cls].sample(min(50, len(df[df[0] == cls]))) for cls in class_list.index])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "33I4JsOrM2xZ"
   },
   "outputs": [],
   "source": [
    "# fonction pour le f1 score\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_pred_labels = tf.argmax(y_pred, axis=1)\n",
    "    y_true_labels = tf.cast(y_true, tf.int64)\n",
    "    f1 = tf.py_function(lambda y_true, y_pred_labels: f1_score(y_true, y_pred_labels, average='weighted'), (y_true_labels, y_pred_labels), tf.float64)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour lire le fichier csv du test et le mettre dans un dataframe\n",
    "def read_file_test(name):\n",
    "    with open(name, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        data_brut = []\n",
    "        for line in lines:\n",
    "            # on verifie que chaque ligne commence par une de ces trois valeurs\n",
    "            if line.startswith('Terran') or line.startswith('Protoss') or line.startswith('Zerg'):\n",
    "                values = line.strip().split(\",\")\n",
    "                data_brut.append(values)\n",
    "            else : continue\n",
    "    df = pd.DataFrame(data_brut)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour decoder et ajouter les bonnes colonnes aux predictions\n",
    "def predictions(name,model,X_test_short):\n",
    "    # on fait notre predciton\n",
    "    y_pred = pd.DataFrame(model.predict(X_test_short))\n",
    "\n",
    "    # on rajoute un index\n",
    "    max_col_index = y_pred.idxmax(axis=1)\n",
    "\n",
    "    # on importe notre dictionnaire d'encodage, on le retourne et on decode notre prediction\n",
    "    with open(name, 'r') as f:\n",
    "        dico = json.load(f)\n",
    "        dico = {k: v - 1 for k, v in dico.items()}\n",
    "    dico_inverse = {v: k for k, v in dico.items()}\n",
    "    max_col_index = pd.DataFrame(max_col_index.replace(dico_inverse))\n",
    "\n",
    "    # on renomme les colonnes\n",
    "    max_col_index.columns = ['prediction']\n",
    "    max_col_index['RowId'] = max_col_index.index + 1\n",
    "    max_col_index = max_col_index[['RowId', 'prediction']]\n",
    "    return max_col_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYhm1hjeHj-K"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJGsme_RHlQv"
   },
   "source": [
    "Import des données, on enleve les t, on reduit la dimension du df, on crée un dictionnaire, on l'encode avec ce dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "B7Tild7EPkGt"
   },
   "outputs": [],
   "source": [
    "# import des données d'entrainement\n",
    "df_brut = read_file('TRAIN.CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "dDW2pv12imhR",
    "outputId": "83a6dbeb-9966-46d5-bf33-d4ff6b021019"
   },
   "outputs": [],
   "source": [
    "# enlever les valeurs  de type qui commencent par 't'\n",
    "df_sans_t = sans_t(df_brut,'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QayzVPAevLpC"
   },
   "outputs": [],
   "source": [
    "# suppression des dimensions inutiles suite a la fonction precedente\n",
    "df_avant_encodage = reduction(df_sans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CkqbJnBxIQgn"
   },
   "outputs": [],
   "source": [
    "# creation dictionnaire d'encodage\n",
    "dico = creation_dico(df_avant_encodage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dY_dYhw0z-3A"
   },
   "outputs": [],
   "source": [
    "# enregistrement du dictionnaire d'encodage\n",
    "with open('dictionnaire_encodage.json', 'w') as f:\n",
    "    json.dump(dico, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vD_GufmuJJHm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": [],
   "source": [
    "# encodage du dataframe d'entrainement\n",
    "df_encoded = encodage(df_avant_encodage,dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fmOdp1pTKhbF"
   },
   "outputs": [],
   "source": [
    "# encodage des colonnes qui ont ratée\n",
    "df_encoded = correction(df_avant_encodage,df_encoded,dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy1ILGn3M8_0"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFbDTi1_M-At"
   },
   "source": [
    "Separation des données en X et Y, SMOTE et undersampling, mélange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VIRGMum88kWd"
   },
   "outputs": [],
   "source": [
    "# separation en X et Y pour l'entrainement\n",
    "X,Y = df_encoded.drop(df_encoded.columns[0],axis=1),df_encoded.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JazUInrENrDX"
   },
   "outputs": [],
   "source": [
    "# appel de la fonction SMOTE pour de l'oversampling\n",
    "XY_smote = smote(Y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cv4WD7VlOPAz"
   },
   "outputs": [],
   "source": [
    "# appel de la fonctin UNDER pour de l'undersampling\n",
    "XY_smote = under(XY_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bkZX5nvtLvNf"
   },
   "outputs": [],
   "source": [
    "# Nouvelle separation pour donnée finale d'entrainement\n",
    "X_tot_smote, Y_tot_smote = XY_smote.drop(df_encoded.columns[0],axis=1), XY_smote[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r0usBs6xMdVW"
   },
   "outputs": [],
   "source": [
    "# enregistrement des données traitées pour l'entrainement\n",
    "X_tot_smote.to_csv('X_tot_smote_bon.csv', index=False)\n",
    "Y_tot_smote.to_csv('Y_tot_smote_bon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m81LtbQTO1CZ"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsBCpxBgO1zZ"
   },
   "source": [
    "traitement des données pour le modele, reduction de dimension, division des données, creation du modele, entrainement, et visualisation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_6YNuu9mMixV"
   },
   "outputs": [],
   "source": [
    "# reduction de la dimension pour simplifier l'entrainement du modele\n",
    "dim_max = 150\n",
    "X_short = X_tot_smote.iloc[:,1:dim_max+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CNI-rGQRMrGF"
   },
   "outputs": [],
   "source": [
    "# separation des données pour l'entrainement et tester l'entrainement\n",
    "x_train,x_temp,y_train,y_temp = train_test_split(X_short, Y_tot_smote, test_size = 0.1, random_state = 42)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_temp,y_temp, train_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A_9GGT97Ms4E"
   },
   "outputs": [],
   "source": [
    "# initialisation du modele : \n",
    "    # couche d'embelling pour les dimensions des données d'entrées\n",
    "    # couche de normalisation \n",
    "    # premiere couche de 1024 neurones\n",
    "    # dropout pour eviter la rigidé du modele\n",
    "    # deuxieme couche de 512 neurones\n",
    "    # couche Dense avec 200 neurones pour nos 200 joueurs\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=238, output_dim=128, input_length=dim_max),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(1024, return_sequences = True, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(512, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(200, activation='softmax')\n",
    "])\n",
    "\n",
    "# compliation du modele avec un taux d'apprentissage en decroissance exponentielle\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=[f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0Kj5947YR7a",
    "outputId": "089bafdf-4b4d-47b0-c3ac-5a7287d32812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 869s 12s/step - loss: 0.7929 - f1_metric: 0.8948 - val_loss: 1.0434 - val_f1_metric: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28384846140>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrainement de notre modele \n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m81LtbQTO1CZ"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des données de prédictions, prédiction et enregistrement du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import du fichier à prédire\n",
    "X_prediction = read_file_test('TEST.CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des valeurs commencant par 't'\n",
    "X_prediction_clean = sans_t(X_prediction,'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# réduction du nombre de dimensions du à la fonction precedente \n",
    "X_pred_clean = reduction(X_prediction_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dico' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# encodage \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_pred_encoded \u001b[38;5;241m=\u001b[39m encodage(X_pred_clean,\u001b[43mdico\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dico' is not defined"
     ]
    }
   ],
   "source": [
    "# encodage du fichier à predire\n",
    "X_pred_encoded = encodage(X_pred_clean,dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction de la dimension pour etre conforme aux données d'entrianement du modele\n",
    "X_pred_encoded_short = X_pred_encoded.iloc[:,1:dim_max+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 19s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# prediction et formatage du fichier final\n",
    "pred = predictions('dictionnaire_encodage.json',model,X_pred_encoded_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "0ZSH-m31UX70"
   },
   "outputs": [],
   "source": [
    "# enregistrement en csv du fichier\n",
    "pred.to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
